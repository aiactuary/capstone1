[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aiactuary/capstone1/blob/main/capstone_lapse_eda.ipynb)
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff8f31f",
   "metadata": {},
   "source": [
    "# Capstone EDA & Baseline Model — Policyholder Lapse (Churn Proxy)\n",
    "*Last updated:* 2025-08-24 22:08\n",
    "\n",
    "**Research question:** Can syllabus-aligned ML classifiers predict which policyholders are likely to lapse within 12 months, enabling actionable risk tiers for retention?\n",
    "\n",
    "**Data:** **Telco Customer Churn (Kaggle)** — download the CSV and place it in the repo root or `data/` folder.\n",
    "\n",
    "> **Repo hygiene:** Keep only this notebook, `README.md`, and minimal `requirements.txt`. Avoid committing large raw data; link Kaggle instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay, brier_score_loss)\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('talk')  # larger fonts for readability\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83996b0c",
   "metadata": {},
   "source": [
    "## Data Access via Kaggle (Auto-Download)\n",
    "\n",
    "This notebook will **download the Telco Customer Churn dataset automatically** using the Kaggle API.\n",
    "\n",
    "**Authentication (one-time):**\n",
    "- Preferred: place your `kaggle.json` (from your Kaggle account) at `~/.kaggle/kaggle.json` on your machine.\n",
    "- Or set environment variables before running:  \n",
    "  `KAGGLE_USERNAME=your_username` and `KAGGLE_KEY=your_key`\n",
    "\n",
    "> Your credentials stay on your machine. This notebook simply calls the official Kaggle API to fetch the file into a local `./data` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063470c0",
   "metadata": {},
   "source": [
    "### Colab users: set Kaggle credentials\n",
    "\n",
    "If you're running in **Google Colab** and don't have `~/.kaggle/kaggle.json` set up, run the cell below to upload it.\n",
    "You can generate the token at Kaggle → Account → \"Create New API Token\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab helper: upload kaggle.json interactively (skip if you already have ~/.kaggle/kaggle.json)\n",
    "import os, json, pathlib\n",
    "\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files  # type: ignore\n",
    "    kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
    "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Please upload your kaggle.json (downloaded from Kaggle account settings).\")\n",
    "    uploaded = files.upload()  # prompts for file upload\n",
    "    if 'kaggle.json' in uploaded:\n",
    "        with open(kaggle_dir / \"kaggle.json\", \"wb\") as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "        os.chmod(kaggle_dir / \"kaggle.json\", 0o600)\n",
    "        print(\"kaggle.json saved to ~/.kaggle/kaggle.json\")\n",
    "    else:\n",
    "        print(\"kaggle.json not uploaded. If you already have credentials configured, you can ignore this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d58a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and configure Kaggle API, then download the dataset\n",
    "import os, json, pathlib, sys, subprocess\n",
    "\n",
    "def _pip_install(package):\n",
    "    print(f\"Installing {package} ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# 1) Ensure kaggle is installed\n",
    "try:\n",
    "    import kaggle  # noqa: F401\n",
    "except Exception:\n",
    "    _pip_install(\"kaggle\")\n",
    "    import kaggle  # noqa: F401\n",
    "\n",
    "# 2) Ensure credentials are available\n",
    "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
    "kaggle_json = kaggle_dir / \"kaggle.json\"\n",
    "k_user = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "k_key = os.environ.get(\"KAGGLE_KEY\")\n",
    "\n",
    "if not kaggle_json.exists():\n",
    "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if k_user and k_key:\n",
    "        print(\"Creating ~/.kaggle/kaggle.json from environment variables ...\")\n",
    "        kaggle_json.write_text(json.dumps({\"username\": k_user, \"key\": k_key}), encoding=\"utf-8\")\n",
    "    else:\n",
    "        print(\"WARNING: kaggle.json not found and KAGGLE_USERNAME/KAGGLE_KEY not set.\")\n",
    "        print(\"Please place your kaggle.json at ~/.kaggle/kaggle.json or set KAGGLE_USERNAME and KAGGLE_KEY.\")\n",
    "# Secure permissions (Kaggle API requires 600)\n",
    "try:\n",
    "    os.chmod(kaggle_json, 0o600)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 3) Download and unzip dataset into ./data\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Use Kaggle CLI via subprocess (more robust inside notebooks)\n",
    "print(\"Downloading Telco Customer Churn dataset from Kaggle ...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"kaggle\", \"datasets\", \"download\", \"-d\",\n",
    "                       \"blastchar/telco-customer-churn\", \"-p\", str(data_dir), \"-f\", \"WA_Fn-UseC_-Telco-Customer-Churn.csv\", \"--force\"])\n",
    "\n",
    "# Unzip if a zip was downloaded (Kaggle may deliver zip if file flag not respected)\n",
    "for p in data_dir.glob(\"*.zip\"):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(p, 'r') as zf:\n",
    "        zf.extractall(data_dir)\n",
    "    p.unlink()\n",
    "\n",
    "# 4) Determine CSV path\n",
    "candidates = [\n",
    "    data_dir / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\",\n",
    "    data_dir / \"Telco-Customer-Churn.csv\"\n",
    "]\n",
    "data_path = None\n",
    "for c in candidates:\n",
    "    if c.exists():\n",
    "        data_path = c\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    # fallback: search any CSV in data/\n",
    "    csvs = list(data_dir.glob(\"*.csv\"))\n",
    "    if csvs:\n",
    "        data_path = csvs[0]\n",
    "\n",
    "print(f\"Using dataset: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ec1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Kaggle-downloaded path\n",
    "import pandas as pd\n",
    "\n",
    "if 'data_path' not in globals() or data_path is None:\n",
    "    raise FileNotFoundError(\"Dataset not found. Run the Kaggle download cell above first.\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af099df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure\n",
    "rows, cols = df.shape\n",
    "print(f'Rows: {rows}, Columns: {cols}')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995a989",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TotalCharges to numeric; impute blanks from MonthlyCharges\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    if 'MonthlyCharges' in df.columns:\n",
    "        df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n",
    "\n",
    "# Drop duplicate customerIDs if any\n",
    "if 'customerID' in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=['customerID'])\n",
    "    print('Removed duplicates:', before - len(df))\n",
    "\n",
    "# Standardize target\n",
    "df['Churn'] = df['Churn'].astype(str).str.strip().str.title()  # 'Yes'/'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30ab31",
   "metadata": {},
   "source": [
    "## Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3426c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = (df['Churn'] == 'Yes').astype(int)\n",
    "ax = y_bin.value_counts().sort_index().plot(kind='bar')\n",
    "ax.set_xticklabels(['No','Yes'])\n",
    "ax.set_title('Lapse (Churn) Distribution')\n",
    "ax.set_xlabel('Churn')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()\n",
    "print('Churn rate:', y_bin.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd490c",
   "metadata": {},
   "source": [
    "## Advanced EDA: Numeric & Categorical Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4124e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric distributions split by churn\n",
    "num_preview = [c for c in ['tenure','MonthlyCharges','TotalCharges'] if c in df.columns]\n",
    "fig, axes = plt.subplots(1, len(num_preview), figsize=(6*len(num_preview), 5))\n",
    "if len(num_preview) == 1:\n",
    "    axes = [axes]\n",
    "for ax, col in zip(axes, num_preview):\n",
    "    sns.histplot(data=df, x=col, hue='Churn', kde=True, stat='density', common_norm=False, ax=ax)\n",
    "    ax.set_title(f'Distribution of {col} by Churn')\n",
    "    ax.set_xlabel(col); ax.set_ylabel('Density')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical stacked bars vs churn\n",
    "for col in [c for c in ['Contract','PaymentMethod','PaperlessBilling','InternetService'] if c in df.columns]:\n",
    "    ct = (pd.crosstab(df[col], df['Churn'], normalize='index')*100).round(1)\n",
    "    ct.plot(kind='bar', stacked=True)\n",
    "    plt.title(f'{col} vs Churn (row %)')\n",
    "    plt.ylabel('Percentage'); plt.legend(title='Churn')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for outliers\n",
    "for c in [col for col in ['MonthlyCharges','TotalCharges','tenure'] if col in df.columns]:\n",
    "    df.boxplot(column=c, by='Churn', grid=False)\n",
    "    plt.suptitle(''); plt.title(f'Boxplot of {c} by Churn')\n",
    "    plt.xlabel('Churn'); plt.ylabel(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (numeric)\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "if len(num_cols) > 1:\n",
    "    corr = df[num_cols].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar=True)\n",
    "    plt.title('Correlation Heatmap (Numeric)')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb646b99",
   "metadata": {},
   "source": [
    "## Outlier Analysis (IQR & Z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def outlier_summary(series):\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(s) == 0:\n",
    "        return {'count': 0, 'iqr_outliers': 0, 'iqr_pct': 0.0, 'z_outliers': 0, 'z_pct': 0.0}\n",
    "    q1, q3 = np.percentile(s, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    iqr_low, iqr_high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    iqr_mask = (s < iqr_low) | (s > iqr_high)\n",
    "    mu, sd = s.mean(), s.std(ddof=0)\n",
    "    z_mask = (np.abs((s - mu) / (sd if sd>0 else 1)) > 3)\n",
    "    return {'count': int(s.shape[0]), 'iqr_outliers': int(iqr_mask.sum()), 'iqr_pct': float(iqr_mask.mean()*100),\n",
    "            'z_outliers': int(z_mask.sum()), 'z_pct': float(z_mask.mean()*100)}\n",
    "\n",
    "numeric_focus = [c for c in ['MonthlyCharges','TotalCharges','tenure'] if c in df.columns]\n",
    "outlier_report = {c: outlier_summary(df[c]) for c in numeric_focus}\n",
    "pd.DataFrame(outlier_report).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c2070",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service count\n",
    "service_cols = [c for c in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies'] if c in df.columns]\n",
    "if service_cols:\n",
    "    df['ServiceCount'] = df[service_cols].apply(lambda r: sum(x=='Yes' for x in r), axis=1)\n",
    "else:\n",
    "    df['ServiceCount'] = 0\n",
    "\n",
    "# Tenure band\n",
    "if 'tenure' in df.columns:\n",
    "    bins = [0, 6, 12, 24, 48, 72, 1000]\n",
    "    labels = ['0-6','6-12','12-24','24-48','48-72','72+']\n",
    "    df['tenure_band'] = pd.cut(df['tenure'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# AutoPay\n",
    "if 'PaymentMethod' in df.columns:\n",
    "    df['AutoPay'] = df['PaymentMethod'].str.contains('automatic', case=False, na=False).map({True:'Yes', False:'No'})\n",
    "\n",
    "# ChargesRatio\n",
    "if set(['MonthlyCharges','TotalCharges','tenure']).issubset(df.columns):\n",
    "    denom = (df['tenure'].clip(lower=1) * df['MonthlyCharges']).replace(0, np.nan)\n",
    "    df['ChargesRatio'] = (pd.to_numeric(df['TotalCharges'], errors='coerce') / denom).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "# Interaction: high monthly & month-to-month\n",
    "if 'MonthlyCharges' in df.columns and 'Contract' in df.columns:\n",
    "    high_m = df['MonthlyCharges'] > df['MonthlyCharges'].median()\n",
    "    mtm = df['Contract'].eq('Month-to-month')\n",
    "    df['HighMonthly_MTM'] = np.where(high_m & mtm, 'Yes', 'No')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be149f",
   "metadata": {},
   "source": [
    "## Evaluation Metric Rationale (Why ROC‑AUC, plus Precision/Recall)\n",
    "\n",
    "- **Class imbalance** (~26% lapses): **accuracy** can be misleading.\n",
    "- **Primary metric: ROC‑AUC** — measures ranking quality across thresholds → ideal for **risk tiering**.\n",
    "- **Secondary:** Precision/Recall/F1 at 0.5 and a **tuned threshold** (Youden’s J) to show business trade‑offs.\n",
    "- **Calibration:** Brier score + curve ensure probabilities are usable for decisioning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d100b",
   "metadata": {},
   "source": [
    "## Baseline & Model Comparison (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49540561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features/target\n",
    "y = (df['Churn'] == 'Yes').astype(int)\n",
    "X = df.drop(columns=['Churn','customerID'], errors='ignore')\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'LogReg': LogisticRegression(max_iter=500, solver='liblinear'),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([('prep', preprocess), ('clf', clf)])\n",
    "    aucs = cross_val_score(pipe, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_results[name] = {'ROC_AUC_mean': aucs.mean(), 'ROC_AUC_std': aucs.std()}\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).T.sort_values('ROC_AUC_mean', ascending=False)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test champion\n",
    "champ_name = cv_df.index[0]\n",
    "champ = models[champ_name]\n",
    "pipe = Pipeline([('prep', preprocess), ('clf', champ)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7, stratify=y)\n",
    "pipe.fit(X_train, y_train)\n",
    "proba = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Metrics @ 0.5\n",
    "pred05 = (proba >= 0.5).astype(int)\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "acc = accuracy_score(y_test, pred05)\n",
    "prec = precision_score(y_test, pred05)\n",
    "rec = recall_score(y_test, pred05)\n",
    "\n",
    "# Calibration & Brier\n",
    "frac_pos, mean_pred = calibration_curve(y_test, proba, n_bins=10, strategy='quantile')\n",
    "brier = brier_score_loss(y_test, proba)\n",
    "\n",
    "# Threshold tuning (Youden's J)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thr = roc_curve(y_test, proba)\n",
    "youden = tpr - fpr\n",
    "best_idx = np.argmax(youden)\n",
    "best_thr = thr[best_idx]\n",
    "pred_tuned = (proba >= best_thr).astype(int)\n",
    "\n",
    "def metrics_dict(label, y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'Label': label,\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_prob),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame([\n",
    "    metrics_dict('0.5 threshold', y_test, pred05, proba),\n",
    "    metrics_dict(f'Tuned (YoudenJ={best_thr:.3f})', y_test, pred_tuned, proba)\n",
    "])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b460d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual diagnostics\n",
    "fig, axes = plt.subplots(1,3, figsize=(18,5))\n",
    "RocCurveDisplay.from_predictions(y_test, proba, ax=axes[0]); axes[0].set_title(f'ROC — {champ_name}')\n",
    "PrecisionRecallDisplay.from_predictions(y_test, proba, ax=axes[1]); axes[1].set_title(f'PR — {champ_name}')\n",
    "axes[2].plot(mean_pred, frac_pos, marker='o', label='Model'); axes[2].plot([0,1],[0,1],'--', label='Perfect')\n",
    "axes[2].set_title(f'Calibration (Brier={brier:.3f})'); axes[2].set_xlabel('Mean predicted'); axes[2].set_ylabel('Fraction positive'); axes[2].legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(confusion_matrix(y_test, pred05), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix @ 0.5'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, pred_tuned), annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix @ Tuned ({best_thr:.3f})'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e4034",
   "metadata": {},
   "source": [
    "## Export Key Results to README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ce352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after training to auto-write CV and test metrics to README.md\n",
    "readme = Path('README.md')\n",
    "lines = []\n",
    "lines.append('# Capstone — Policyholder Lapse Prediction (Churn Proxy)\\n')\n",
    "lines.append('\\n**Module 20.1 — Initial Report & EDA** (auto-updated)\\n\\n')\n",
    "lines.append('## Results (Run on Telco Customer Churn)\\n')\n",
    "lines.append(f'- Champion model: **{champ_name}** (by CV ROC-AUC)\\n')\n",
    "lines.append('- Cross-validated ROC-AUC (5-fold):\\n')\n",
    "for name, row in cv_df.iterrows():\n",
    "    lines.append(f'  - {name}: {row.ROC_AUC_mean:.3f} ± {row.ROC_AUC_std:.3f}\\n')\n",
    "lines.append('\\n### Test Set Metrics\\n')\n",
    "for _, r in metrics_df.iterrows():\n",
    "    lines.append(f\"- {r['Label']} → AUC: {r['ROC_AUC']:.3f}, Acc: {r['Accuracy']:.3f}, Prec: {r['Precision']:.3f}, Rec: {r['Recall']:.3f}, F1: {r['F1']:.3f}\\n\")\n",
    "lines.append(f'\\nCalibration (Brier): **{brier:.3f}**\\n')\n",
    "lines.append('\\n*Note:* ROC-AUC is primary due to imbalance; tuned threshold (Youden\\'s J) balances TPR/FPR for retention.\\n')\n",
    "readme.write_text(''.join(lines), encoding='utf-8')\n",
    "print('README.md updated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a92b6e",
   "metadata": {},
   "source": [
    "## Rubric Checklist (Module 20.1)\n",
    "\n",
    "- **Project organization:** Clear headings; clean sections; link to notebook in README; no extraneous large outputs.\n",
    "- **Syntax & code quality:** Standard imports/aliases; comments; sensible variables; no massive dumps.\n",
    "- **Visualizations:** Numeric (hist/KDE), categorical (stacked bars), boxplots, correlation heatmap, subplots, legible titles/labels.\n",
    "- **Data cleaning & EDA:** Missing values imputed; duplicates dropped; **outlier analysis (IQR & z-score)**; feature engineering (tenure_band, AutoPay, ChargesRatio, ServiceCount, interaction).\n",
    "- **Modeling:** Baseline Logistic Regression identified; **ROC‑AUC** chosen with rationale; precision/recall/F1 reported; calibration/brier; tuned threshold; confusion matrices.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
