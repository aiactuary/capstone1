{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e4330a",
   "metadata": {},
   "source": [
    "# Capstone EDA & Baseline — Policyholder Lapse (Churn Proxy)\n",
    "*Last updated:* 2025-08-24 22:48\n",
    "\n",
    "**Research question:** Can syllabus‑aligned ML classifiers (baseline Logistic Regression) predict which policyholders are likely to lapse within 12 months and support actionable **risk tiers** for retention?\n",
    "\n",
    "**Dataset (no login required):** **UCI ML Repository — Telco Customer Churn**  \n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00470/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "\n",
    "> This notebook is designed to satisfy **Module 20.1** requirements: data cleaning, EDA/visualizations, feature engineering, baseline model + evaluation, and an executive brief.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay, brier_score_loss)\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('talk')\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100955ea",
   "metadata": {},
   "source": [
    "## Load Data (UCI — direct CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeefd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "UCI_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00470/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(UCI_URL)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e40a46",
   "metadata": {},
   "source": [
    "## Basic Structure & Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = df.shape\n",
    "print(f'Rows: {rows}, Columns: {cols}')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06257cb2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TotalCharges to numeric (blanks to NaN), impute from MonthlyCharges if missing\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    if 'MonthlyCharges' in df.columns:\n",
    "        df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n",
    "\n",
    "# Drop duplicate customers if any\n",
    "if 'customerID' in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=['customerID'])\n",
    "    print('Removed duplicates:', before - len(df))\n",
    "\n",
    "# Standardize target labels\n",
    "df['Churn'] = df['Churn'].astype(str).str.strip().str.title()  # 'Yes'/'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d41a7",
   "metadata": {},
   "source": [
    "## Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df['Churn'] == 'Yes').astype(int)\n",
    "ax = y.value_counts().sort_index().plot(kind='bar')\n",
    "ax.set_xticklabels(['No','Yes'])\n",
    "ax.set_title('Lapse (Churn) Distribution')\n",
    "ax.set_xlabel('Churn'); ax.set_ylabel('Count')\n",
    "plt.show()\n",
    "print('Churn rate:', y.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14290b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Numeric distributions split by churn\n",
    "num_preview = [c for c in ['tenure','MonthlyCharges','TotalCharges'] if c in df.columns]\n",
    "fig, axes = plt.subplots(1, len(num_preview), figsize=(6*len(num_preview), 5))\n",
    "if len(num_preview) == 1: axes = [axes]\n",
    "for ax, col in zip(axes, num_preview):\n",
    "    sns.histplot(data=df, x=col, hue='Churn', kde=True, stat='density', common_norm=False, ax=ax)\n",
    "    ax.set_title(f'{col} by Churn'); ax.set_xlabel(col); ax.set_ylabel('Density')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Categorical stacked bars vs churn\n",
    "for col in [c for c in ['Contract','PaymentMethod','PaperlessBilling','InternetService'] if c in df.columns]:\n",
    "    ct = (pd.crosstab(df[col], df['Churn'], normalize='index')*100).round(1)\n",
    "    ct.plot(kind='bar', stacked=True)\n",
    "    plt.title(f'{col} vs Churn (row %)'); plt.ylabel('Percentage'); plt.legend(title='Churn')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Boxplots for outliers\n",
    "for c in [col for col in ['MonthlyCharges','TotalCharges','tenure'] if col in df.columns]:\n",
    "    df.boxplot(column=c, by='Churn', grid=False)\n",
    "    plt.suptitle(''); plt.title(f'Boxplot of {c} by Churn'); plt.xlabel('Churn'); plt.ylabel(c)\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap (numeric)\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "if len(num_cols) > 1:\n",
    "    corr = df[num_cols].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar=True)\n",
    "    plt.title('Correlation Heatmap (Numeric)')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd8f31",
   "metadata": {},
   "source": [
    "## Outlier Analysis (IQR & Z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe23d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def outlier_summary(series):\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(s) == 0:\n",
    "        return {'count': 0, 'iqr_outliers': 0, 'iqr_pct': 0.0, 'z_outliers': 0, 'z_pct': 0.0}\n",
    "    q1, q3 = np.percentile(s, [25, 75]); iqr = q3 - q1\n",
    "    iqr_low, iqr_high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    iqr_mask = (s < iqr_low) | (s > iqr_high)\n",
    "    mu, sd = s.mean(), s.std(ddof=0); z_mask = (np.abs((s - mu) / (sd if sd>0 else 1)) > 3)\n",
    "    return {'count': int(s.shape[0]), 'iqr_outliers': int(iqr_mask.sum()), 'iqr_pct': float(iqr_mask.mean()*100),\n",
    "            'z_outliers': int(z_mask.sum()), 'z_pct': float(z_mask.mean()*100)}\n",
    "\n",
    "numeric_focus = [c for c in ['MonthlyCharges','TotalCharges','tenure'] if c in df.columns]\n",
    "outlier_report = {c: outlier_summary(df[c]) for c in numeric_focus}\n",
    "pd.DataFrame(outlier_report).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892668b1",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252dd810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service count\n",
    "service_cols = [c for c in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies'] if c in df.columns]\n",
    "df['ServiceCount'] = df[service_cols].apply(lambda r: sum(x=='Yes' for x in r), axis=1) if service_cols else 0\n",
    "\n",
    "# Tenure band\n",
    "if 'tenure' in df.columns:\n",
    "    bins = [0, 6, 12, 24, 48, 72, 1000]\n",
    "    labels = ['0-6','6-12','12-24','24-48','48-72','72+']\n",
    "    df['tenure_band'] = pd.cut(df['tenure'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# AutoPay flag\n",
    "if 'PaymentMethod' in df.columns:\n",
    "    df['AutoPay'] = df['PaymentMethod'].str.contains('automatic', case=False, na=False).map({True:'Yes', False:'No'})\n",
    "\n",
    "# ChargesRatio\n",
    "if set(['MonthlyCharges','TotalCharges','tenure']).issubset(df.columns):\n",
    "    denom = (df['tenure'].clip(lower=1) * df['MonthlyCharges']).replace(0, np.nan)\n",
    "    df['ChargesRatio'] = (pd.to_numeric(df['TotalCharges'], errors='coerce') / denom).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "# Interaction\n",
    "if 'MonthlyCharges' in df.columns and 'Contract' in df.columns:\n",
    "    high_m = df['MonthlyCharges'] > df['MonthlyCharges'].median()\n",
    "    mtm = df['Contract'].eq('Month-to-month')\n",
    "    df['HighMonthly_MTM'] = np.where(high_m & mtm, 'Yes', 'No')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552578f6",
   "metadata": {},
   "source": [
    "## Evaluation Metric Rationale\n",
    "- **Primary:** ROC‑AUC (robust to class imbalance; measures ranking for **risk tiers**).\n",
    "- **Secondary:** Accuracy, Precision, Recall, F1 at default and tuned thresholds.\n",
    "- **Calibration:** Brier score + calibration curve to ensure probability quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5cfe4",
   "metadata": {},
   "source": [
    "## Baseline Model — Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3260461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Features & target\n",
    "y = (df['Churn'] == 'Yes').astype(int)\n",
    "X = df.drop(columns=['Churn','customerID'], errors='ignore')\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "log_reg = Pipeline([('prep', preprocess),\n",
    "                    ('clf', LogisticRegression(max_iter=500, solver='liblinear'))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7, stratify=y)\n",
    "log_reg.fit(X_train, y_train)\n",
    "proba = log_reg.predict_proba(X_test)[:,1]\n",
    "pred05 = (proba >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "metrics_baseline = {\n",
    "    'ROC_AUC': roc_auc_score(y_test, proba),\n",
    "    'Accuracy': accuracy_score(y_test, pred05),\n",
    "    'Precision': precision_score(y_test, pred05),\n",
    "    'Recall': recall_score(y_test, pred05),\n",
    "    'F1': f1_score(y_test, pred05)\n",
    "}\n",
    "metrics_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe15dda",
   "metadata": {},
   "source": [
    "## Calibration & Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "frac_pos, mean_pred = calibration_curve(y_test, proba, n_bins=10, strategy='quantile')\n",
    "brier = brier_score_loss(y_test, proba)\n",
    "\n",
    "# Threshold tuning (Youden's J)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thr = roc_curve(y_test, proba)\n",
    "youden = tpr - fpr\n",
    "best_idx = np.argmax(youden)\n",
    "best_thr = thr[best_idx]\n",
    "pred_tuned = (proba >= best_thr).astype(int)\n",
    "\n",
    "def metric_row(label, y_true, y_pred, y_prob):\n",
    "    return {'Label': label,\n",
    "            'ROC_AUC': roc_auc_score(y_true, y_prob),\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred),\n",
    "            'Recall': recall_score(y_true, y_pred),\n",
    "            'F1': f1_score(y_true, y_pred)}\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame([\n",
    "    metric_row('0.5 threshold', y_test, pred05, proba),\n",
    "    metric_row(f'Tuned (YoudenJ={best_thr:.3f})', y_test, pred_tuned, proba)\n",
    "])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: ROC, PR, Calibration, Confusion matrices\n",
    "fig, axes = plt.subplots(1,3, figsize=(18,5))\n",
    "RocCurveDisplay.from_predictions(y_test, proba, ax=axes[0]); axes[0].set_title('ROC — Logistic Regression')\n",
    "PrecisionRecallDisplay.from_predictions(y_test, proba, ax=axes[1]); axes[1].set_title('PR — Logistic Regression')\n",
    "axes[2].plot(mean_pred, frac_pos, marker='o', label='Model'); axes[2].plot([0,1],[0,1],'--', label='Perfect')\n",
    "axes[2].set_title(f'Calibration (Brier={brier:.3f})'); axes[2].set_xlabel('Mean predicted'); axes[2].set_ylabel('Fraction positive'); axes[2].legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(confusion_matrix(y_test, pred05), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix @ 0.5'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, pred_tuned), annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix @ Tuned ({best_thr:.3f})'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ec07b",
   "metadata": {},
   "source": [
    "## (Optional) Model Comparison (Syllabus‑Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e672011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models by 5-fold CV ROC-AUC (kept optional for rubric; logistic remains baseline)\n",
    "models = {\n",
    "    'LogReg': LogisticRegression(max_iter=500, solver='liblinear'),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "preprocess = ColumnTransformer([('num', StandardScaler(), num_cols),\n",
    "                                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)])\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([('prep', preprocess), ('clf', clf)])\n",
    "    aucs = cross_val_score(pipe, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_results[name] = {'ROC_AUC_mean': aucs.mean(), 'ROC_AUC_std': aucs.std()}\n",
    "\n",
    "pd.DataFrame(cv_results).T.sort_values('ROC_AUC_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f58bb3",
   "metadata": {},
   "source": [
    "## Executive Brief (Initial Findings — Paste into README/Word)\n",
    "\n",
    "**Problem**  \n",
    "Lapses (policy cancellations) erode profitability: acquisition costs are sunk while expected premiums vanish. We predicted lapse risk to prioritize retention outreach.\n",
    "\n",
    "**Data & Method**  \n",
    "We used the public Telco Churn dataset (UCI). Baseline **Logistic Regression** (syllabus) predicts lapse probability; we validated via train/test split, calibration, and threshold tuning, focusing on **ROC‑AUC** due to class imbalance.\n",
    "\n",
    "**Key Drivers**  \n",
    "Short tenure, month‑to‑month contracts, electronic check payment, higher monthly charges, and fewer add‑on services increase lapse risk (directionally consistent with industry intuition).\n",
    "\n",
    "**Business Impact**  \n",
    "Ranking customers by probability allows **risk tiers** (e.g., top 10–20%) for targeted retention campaigns. Threshold tuning balances catching more lapsers vs. minimizing unnecessary outreach.\n",
    "\n",
    "**Next Actions**  \n",
    "Pilot retention offers for high‑risk cohorts; evaluate lift; consider ensemble models and cost‑aware thresholds in the next module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6672e",
   "metadata": {},
   "source": [
    "## Export Key Results to README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the notebook, run this cell to append a Results section to your README.md\n",
    "from pathlib import Path\n",
    "\n",
    "readme = Path('README.md')\n",
    "lines = []\n",
    "lines.append('## Results (Run on UCI Telco Churn)\\n')\n",
    "lines.append(f'- Baseline: **Logistic Regression**\\n')\n",
    "lines.append(f'- Test metrics @0.5 → AUC: {metrics_baseline[\"ROC_AUC\"]:.3f}, Acc: {metrics_baseline[\"Accuracy\"]:.3f}, '\n",
    "             f'Prec: {metrics_baseline[\"Precision\"]:.3f}, Rec: {metrics_baseline[\"Recall\"]:.3f}, F1: {metrics_baseline[\"F1\"]:.3f}\\n')\n",
    "lines.append(f'- Tuned threshold ({best_thr:.3f}) → AUC: {metrics_df.iloc[1][\"ROC_AUC\"]:.3f}, Acc: {metrics_df.iloc[1][\"Accuracy\"]:.3f}, '\n",
    "             f'Prec: {metrics_df.iloc[1][\"Precision\"]:.3f}, Rec: {metrics_df.iloc[1][\"Recall\"]:.3f}, F1: {metrics_df.iloc[1][\"F1\"]:.3f}\\n')\n",
    "lines.append(f'\\nCalibration (Brier): **{brier:.3f}**\\n\\n')\n",
    "\n",
    "if readme.exists():\n",
    "    prev = readme.read_text(encoding='utf-8')\n",
    "else:\n",
    "    prev = ''\n",
    "\n",
    "readme.write_text(prev + ('\\n' if not prev.endswith('\\n') else '') + ''.join(lines), encoding='utf-8')\n",
    "print('README.md updated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c7d6a",
   "metadata": {},
   "source": [
    "## Rubric Checklist (Module 20.1)\n",
    "- **Project organization:** Clear headings, explanatory markdown; README with summary, file links.\n",
    "- **Syntax & code quality:** Clean imports, comments, sensible variables; avoid massive printouts.\n",
    "- **Visualizations:** Numeric (hist/KDE), categorical (stacked bar), boxplots, correlation heatmap; readable titles/labels.\n",
    "- **Data Cleaning & EDA:** Convert/impute `TotalCharges`; drop duplicates; **outlier analysis**; feature engineering (`tenure_band`, `AutoPay`, `ChargesRatio`, `ServiceCount`, interaction).\n",
    "- **Modeling:** Baseline **Logistic Regression**; ROC‑AUC rationale; precision/recall/F1; calibration + threshold tuning; confusion matrices.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
